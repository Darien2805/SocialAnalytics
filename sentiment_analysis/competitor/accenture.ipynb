{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ZY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "dict_tagged_sentences = ''\n",
    "# Below indicates the relative path to\n",
    "# positive/negative/inverter/incrementer/decrementer files\n",
    "DICTIONARY_DIR_PREFIX = '../dicts/'\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Accenture Innovation Challenge !!!!!    What's in for you?  1. Pre-Placement Interviews/Offers  2. Winning team will get 1,00,000 INR  3. First Runner Up will get 75, 000 INR  4. Second Runner Up will get 75,000 INR  5. Jury's Choice will get 25,000 per team member  6. Next 5 Runner Ups will get 20, 000 INR    You can apply as an individual or in a team as well :)    Eligibility Year: All Undergraduates from all branches    Ohh man, this is a great opportunity which can land you a job and you can get some great prizes as well(Coupons nhi cash prize hai xd)    APPLY NOW:\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dell = pd.read_csv('../../../scrapy-hackathon/webscrape/linkedin/linkedin_AccentureInnovationChallenge.csv')\n",
    "sentiment_analysis_accenture = pd.DataFrame(dell)\n",
    "sentiment_analysis_accenture = sentiment_analysis_accenture['post']\n",
    "sentiment_analysis_accenture[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "\n",
    "class DictionaryTagger(object):\n",
    "    def __init__(self, dictionary_paths):\n",
    "        \"\"\"\n",
    "\n",
    "        :rtype : object\n",
    "        \"\"\"\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "        dictionaries = [yaml.full_load(dict_file) for dict_file in files]\n",
    "        #print(dictionaries)\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                #print(key)\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    #print(len(key))\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n",
    "\n",
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def sentiment_score(review):\n",
    "    return sum ([value_of(tag) for sentence in dict_tagged_sentences for token in sentence for tag in token[2]])\n",
    "\n",
    "def sentence_score(sentence_tokens, previous_token, acum_score):\n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentiment_score(sentences):\n",
    "    return sum([sentence_score(sentence, None, 0.0) for sentence in sentences])\n",
    "\n",
    "\n",
    "def run_analysis(text):\n",
    "    splitter = Splitter() # This boy will split a long single string into sentences.\n",
    "    postagger = POSTagger() # This boy is the Part-Of-Speech tagger.\n",
    "\n",
    "    # If text contains multiple sentences, this line splits it into individual sentences.\n",
    "    splitted_sentences = splitter.split(text)\n",
    "    #print (splitted_sentences)\n",
    "    #exit(1)\n",
    "\n",
    "    #print (\"########## This performs Part-Of-Speech tagging. ##########\")\n",
    "    # This performs Part-Of-Speech tagging.\n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    #pprint (pos_tagged_sentences)\n",
    "    #exit(1)\n",
    "\n",
    "    # print (\"########## This line loads Positive word and Negative word lexicons. ##########\")\n",
    "    # # This line loads Positive word and Negative word lexicons.\n",
    "    # dicttagger = DictionaryTagger([ DICTIONARY_DIR_PREFIX + 'positive.yml', DICTIONARY_DIR_PREFIX + 'negative.yml'])\n",
    "    # dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    # print(dict_tagged_sentences)\n",
    "    # #exit(1)\n",
    "\n",
    "    # print (\"########## [Baseline Analysis] Using only Positive/Negative lexicon. ##########\")\n",
    "    # score = sentiment_score(dict_tagged_sentences)\n",
    "    # print (\"Score: %d\" % score)\n",
    "    # #exit(1)\n",
    "\n",
    "    # print (\"########## This line loads Positve/Negative lexicon + incrementer/decrementer lexicon. ##########\")\n",
    "    # dicttagger = DictionaryTagger([ DICTIONARY_DIR_PREFIX + 'positive.yml', DICTIONARY_DIR_PREFIX + 'negative.yml', DICTIONARY_DIR_PREFIX + 'inc.yml', DICTIONARY_DIR_PREFIX + 'dec.yml'])\n",
    "    # dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    # pprint(dict_tagged_sentences)\n",
    "    # score = sentiment_score(dict_tagged_sentences)\n",
    "    # print (\"Score: %d\" % score)\n",
    "    # #exit(1)\n",
    "\n",
    "    #print (\"########## This line loads Positve/Negative lexicon + incrementer/decrementer/inverter lexicon. ##########\")\n",
    "    dicttagger = DictionaryTagger([ DICTIONARY_DIR_PREFIX + 'positive.yml', DICTIONARY_DIR_PREFIX + 'negative.yml', DICTIONARY_DIR_PREFIX + 'inc.yml', DICTIONARY_DIR_PREFIX + 'dec.yml', DICTIONARY_DIR_PREFIX + 'inv.yml'])\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    #pprint(dict_tagged_sentences)\n",
    "    score = sentiment_score(dict_tagged_sentences)\n",
    "    #print (\"Score: %d\" % score)\n",
    "    return score\n",
    "    #exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### This is the MAIN section ###################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #print (\"###############################\")\n",
    "\n",
    "    sentiment_array = []\n",
    "    total_score = 0\n",
    "    length = len(sentiment_analysis_accenture)\n",
    "    for post in sentiment_analysis_accenture:\n",
    "        #print (\"###############################\")\n",
    "        # Run sentiment scoring\n",
    "        score = run_analysis(post)\n",
    "        if (score == 0):\n",
    "            sentiment_array.append(\"Neutral\")\n",
    "        elif (score > 0 and score < 6):\n",
    "            sentiment_array.append(\"Positive\")\n",
    "        elif (score >= 6):\n",
    "            sentiment_array.append(\"Very Positive\")\n",
    "        elif (score < 0):\n",
    "            sentiment_array.append(\"Negative\")\n",
    "        total_score += score\n",
    "    average_score = total_score/length\n",
    "    # Run sentiment scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.DataFrame(sentiment_array, columns=['Sentiment Score'])\n",
    "sentiment_df.to_csv('accenture_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Very Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Very Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Very Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Very Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiment Score\n",
       "0          Positive\n",
       "1     Very Positive\n",
       "2     Very Positive\n",
       "3          Positive\n",
       "4     Very Positive\n",
       "..              ...\n",
       "103   Very Positive\n",
       "104   Very Positive\n",
       "105        Positive\n",
       "106   Very Positive\n",
       "107   Very Positive\n",
       "\n",
       "[108 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d200baa17359a87e61a27565414f94ebaac2390fa574b7650a1efb5cd2bad58c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
