{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ZY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "dict_tagged_sentences = ''\n",
    "# Below indicates the relative path to\n",
    "# positive/negative/inverter/incrementer/decrementer files\n",
    "DICTIONARY_DIR_PREFIX = '../dicts/'\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Asia's Largest, Prized Hackathon Organised by ...\n",
       "1      Hackathon Terbesar dan Berharga di Asia Disele...\n",
       "2      Asia's Largest, Prized Hackathon Organised by ...\n",
       "3      Asia's Largest, Prized Hackathon Organised by ...\n",
       "4      Asia's Largest, Prized Hackathon Organised by ...\n",
       "                             ...                        \n",
       "496    Why PM Modi liked one specific camera at Singa...\n",
       "497    Join us for Barclays #DerivHack2019 on Oct 16/...\n",
       "498    Join us for Barclays #DerivHack2019 on Oct 16/...\n",
       "499    https://t.co/Hr7xAF3Rml\\r\\nPM Modi attends Sin...\n",
       "500    The Singapore India Hackathon, an initiative o...\n",
       "Name: Tweet, Length: 501, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = pd.read_csv('../../../scrapy-hackathon/webscrape/twitter/twitter_oldtweet.csv')\n",
    "sentiment_analysis_dell = pd.DataFrame(tweet)\n",
    "sentiment_analysis_dell = sentiment_analysis_dell['Tweet']\n",
    "sentiment_analysis_dell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "\n",
    "class DictionaryTagger(object):\n",
    "    def __init__(self, dictionary_paths):\n",
    "        \"\"\"\n",
    "\n",
    "        :rtype : object\n",
    "        \"\"\"\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "        dictionaries = [yaml.full_load(dict_file) for dict_file in files]\n",
    "        #print(dictionaries)\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                #print(key)\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    #print(len(key))\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n",
    "\n",
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def sentiment_score(review):\n",
    "    return sum ([value_of(tag) for sentence in dict_tagged_sentences for token in sentence for tag in token[2]])\n",
    "\n",
    "def sentence_score(sentence_tokens, previous_token, acum_score):\n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentiment_score(sentences):\n",
    "    return sum([sentence_score(sentence, None, 0.0) for sentence in sentences])\n",
    "\n",
    "\n",
    "def run_analysis(text):\n",
    "    splitter = Splitter() # This boy will split a long single string into sentences.\n",
    "    postagger = POSTagger() # This boy is the Part-Of-Speech tagger.\n",
    "\n",
    "    # If text contains multiple sentences, this line splits it into individual sentences.\n",
    "    splitted_sentences = splitter.split(text)\n",
    "    #print (splitted_sentences)\n",
    "    #exit(1)\n",
    "\n",
    "    #print (\"########## This performs Part-Of-Speech tagging. ##########\")\n",
    "    # This performs Part-Of-Speech tagging.\n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    #pprint (pos_tagged_sentences)\n",
    "    #exit(1)\n",
    "\n",
    "    # print (\"########## This line loads Positive word and Negative word lexicons. ##########\")\n",
    "    # # This line loads Positive word and Negative word lexicons.\n",
    "    # dicttagger = DictionaryTagger([ DICTIONARY_DIR_PREFIX + 'positive.yml', DICTIONARY_DIR_PREFIX + 'negative.yml'])\n",
    "    # dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    # print(dict_tagged_sentences)\n",
    "    # #exit(1)\n",
    "\n",
    "    # print (\"########## [Baseline Analysis] Using only Positive/Negative lexicon. ##########\")\n",
    "    # score = sentiment_score(dict_tagged_sentences)\n",
    "    # print (\"Score: %d\" % score)\n",
    "    # #exit(1)\n",
    "\n",
    "    # print (\"########## This line loads Positve/Negative lexicon + incrementer/decrementer lexicon. ##########\")\n",
    "    # dicttagger = DictionaryTagger([ DICTIONARY_DIR_PREFIX + 'positive.yml', DICTIONARY_DIR_PREFIX + 'negative.yml', DICTIONARY_DIR_PREFIX + 'inc.yml', DICTIONARY_DIR_PREFIX + 'dec.yml'])\n",
    "    # dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    # pprint(dict_tagged_sentences)\n",
    "    # score = sentiment_score(dict_tagged_sentences)\n",
    "    # print (\"Score: %d\" % score)\n",
    "    # #exit(1)\n",
    "\n",
    "    #print (\"########## This line loads Positve/Negative lexicon + incrementer/decrementer/inverter lexicon. ##########\")\n",
    "    dicttagger = DictionaryTagger([ DICTIONARY_DIR_PREFIX + 'positive.yml', DICTIONARY_DIR_PREFIX + 'negative.yml', DICTIONARY_DIR_PREFIX + 'inc.yml', DICTIONARY_DIR_PREFIX + 'dec.yml', DICTIONARY_DIR_PREFIX + 'inv.yml'])\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    #pprint(dict_tagged_sentences)\n",
    "    score = sentiment_score(dict_tagged_sentences)\n",
    "    #print (\"Score: %d\" % score)\n",
    "    return score\n",
    "    #exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### This is the MAIN section ###################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #print (\"###############################\")\n",
    "\n",
    "    sentiment_array = []\n",
    "    total_score = 0\n",
    "    length = len(sentiment_analysis_dell)\n",
    "    for post in sentiment_analysis_dell:\n",
    "        #print (\"###############################\")\n",
    "        # Run sentiment scoring\n",
    "        score = run_analysis(post)\n",
    "        if (score == 0):\n",
    "            sentiment_array.append(\"Neutral\")\n",
    "        elif (score > 0 and score < 6):\n",
    "            sentiment_array.append(\"Positive\")\n",
    "        elif (score >= 6):\n",
    "            sentiment_array.append(\"Very Positive\")\n",
    "        elif (score < 0):\n",
    "            sentiment_array.append(\"Negative\")\n",
    "        total_score += score\n",
    "    average_score = total_score/length\n",
    "    # Run sentiment scoring\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.DataFrame(sentiment_array, columns=['Sentiment Score'])\n",
    "sentiment_df.to_csv('sentiment_scores_twitter.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d200baa17359a87e61a27565414f94ebaac2390fa574b7650a1efb5cd2bad58c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
